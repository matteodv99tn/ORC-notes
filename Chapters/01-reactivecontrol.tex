% !TEX root =  ../notes.tex
\chapter{Reactive Controls}
	
\section{Joint-Space Motion Control}
	The trajectory-tracking \textit{control} problem addresses the question of finding the control inputs $\tau$ that make the system follow a reference trajectory. 
	Denoted with $q^{ref}(t)$ the \textit{reference joint trajectory}, the control problem can be loosely defined as:
	\[ \textrm{find } \tau(t) \qquad \textrm{such that } q(t) \simeq q^{ref}(t) \]
	
	\paragraph{PID control} The simplest strategy that we can use to solve this problem is by means of a \textit{Proportional Integrative Derivative} (\textit{PID}) controller:
	\begin{equation} \label{eq:PIDforce}
		\tau(t) = K_p e(t) + K_d \dot e(t) + K_i\int_0^t e(\zeta)\, d\zeta
	\end{equation}
	where $e(t) = q^{ref}(t) - q(t)$ is the \textit{error} between the reference trajectory and the real measured coordinates, and $K_p,K_d,K_i \in \mathds R^{n\times n}$ are respectively the \textit{proportional}, \textit{derivative} and \textit{integral gain matrices}, which must be positive-definite to ensure stability.
	
	As a rule of thumb, such feedback gains are tuned by first setting the proportional gain, following the derivative one, and last the integral part. In general the best tracking performance is achieved by increasing the proportional gain $K_p$: this however leads to a \textit{stiff} system that is usually less safe for human interaction (since high forces are exerted by the robot itself). For this reason more advanced techniques are used to solve the control problem in joint coordinate.
		
	\paragraph{Inverse-dynamics control} Assumed that the system is known and deterministic (hypothesis carried out throughout most of the course), then both $M$ and $h$ in \eqref{eq:dynamic} are known, and can be exploited to improve the control performance. In inverse-dynamics control (also known as \textit{feedback linearization} or \textit{computed torques}), the controller computes a desired acceleration $\ddot q^d$ for the joint coordinates, and the torques are then computed by exploiting the system dynamics \eqref{eq:dynamic}:
	\begin{equation} \label{eq:inversedynamiccontrol}
	\begin{cases}
		\tau & = M \ddot q^d + h \\ 
		\ddot q^d &= \ddot q^{ref} + K_d \dot e + K_p e
	\end{cases}
	\end{equation}
	Using the control law \eqref{eq:inversedynamiccontrol}, the closed-loop dynamics becomes:
	\begin{align*}
		M \ddot q + h & = \tau \\ 
		M \ddot q + h & = M\ddot q^d + h \\
		\ddot q & = \ddot q^d = \ddot q^{ref} + K_d\dot e + K_p e \\
		0 & = \ddot e + K_d\dot e + K_p e
	\end{align*}
	At this point the stability of the system can be analyzed with linear techniques by transforming this differential equation to first order; defining the vector $y = (e,\dot e) \in \mathds R^{2n}$, the system reduces to the following linear dynamics:
	\[ \dot y = A y \qquad \textrm{with } A  = \begin{bmatrix}
		0 & I \\ -K_p & -K_d
	\end{bmatrix}\]
	Linear theory provides us ways to tune $K_p, K_d$ in such a way that the overall matrix be stable (by enforcing that all eigenvalues of $A$ have a negative real part).
	
	Controller \eqref{eq:inversedynamiccontrol} usually leads to less stiff systems (which is typically desirable), but a very accurate model of the systems dynamics is required, i.e. \eqref{eq:dynamic} must be known.
	
	\paragraph{Integral gain} In \eqref{eq:PIDforce} it has been necessary to embed an integral term in the control law to compensate for the effect of gravity (since no system knowledge is required): this effect is nonlinear in the robots configuration and the integral gain aims to compensate such contribution asymptotically.\\
	In \eqref{eq:inversedynamiccontrol} the integral term is instead missing since all gravitational actions are modeled by the bias force $h$, thus it is automatically compensated by the computed torques law ($\ddot q^d$ must depend just on the tracking error and need not compensate for gravity).
	
	
\section{Cartesian-Space Motion Control}	
	Defining the desired behavior of the robot in joint coordinates is typically hard. A much easier way to define it is in terms of Cartesian space motion of the end-effector. Therefore, it is interesting to have control strategies that work directly in Cartesian space. A simple way to achieve this is to transform the reference Cartesian-space trajectory $x^{ref}(t)$ in a reference joint-space trajectory $q^{ref}(t)$ by solving the so-called \emph{inverse-geometry} problem. This problem consists in finding a valid joint configuration $q$ such that the end-effector position (and possibly orientation) matches a desired value. 
	This solution can theoretically work, but it presents several downsides. First, we have to deal with the high nonlinearity of the geometry function, which makes it impossible to find an analytical solution of the problem, even for simple robots. This is particularly important if the reference trajectory cannot be known in advance (e.g., if the end-effector has to track a moving object), and therefore the inverse-geometry problem must be solved inside the control loop.
	
	We can therefore try to design a control law that works directly with a Cartesian-space reference trajectory $x^{ref}(t)$. We want to compute the joint torques $\tau(t)$ such that the end-effector trajectory $x(t) \in \mathds R^3$ (in Cartesian coordinates) follows $x^{ref}(t)$:
	\[ x(t) \simeq x^{ref}(t) \]
	
	\subsection{Operational-Space Control} 
	Let us start by defining the relationship between the end-effector position $x \in \mathds{R}^3$ and the joint configuration $q$, which is the so-called \emph{forward geometry} function:
	\begin{equation}
	x = \text{FG}(q)
	\end{equation}
	By taking the time derivative of this relationship we get:
	\begin{equation}
	\dot x = \frac{d}{dt} \text{FG}(q) = \frac{d \text{FG}(q)}{dq} \frac{dq}{dt} = J(q) \dot q
	\end{equation}
	Taking the time derivative once again:
	\begin{equation}
	\ddot x = J(q) \ddot q + \dot{J}(q, \dot q) \dot q
	\end{equation}
	This relationship between $\ddot x$ and $\ddot q$ will be useful in the following derivation.
	Let us pre-multiplying the systems dynamic \eqref{eq:dynamic} by $JM^{-1}$, to map the equation from joint to Cartesian space:
	\begin{equation} \label{eq:temp:2}
	\begin{split}
		J\cancel{M^{-1}M}\ddot q + JM^{-1} h & = JM^{-1} \tau \\
		\ddot x - \dot J \dot q + JM^{-1}h & =
	\end{split}
	\end{equation}
	where $J \ddot q$ has been substituted by $\ddot x - \dot J \dot q$. 
	Assuming that the Jacobian $J$ is full-row rank (i.e. the end-effector is allowed to move in all directions) then $JM^{-1}J^\top \in \mathds R^{3 \times 3}$ is invertible; pre-multiplying \eqref{eq:temp:2} by the matrix $\Lambda \triangleq \big(JM^{-1}J^\top\big)^{-1}$ gives
	\begin{equation*}
		\Lambda \ddot x + \underbrace{\Lambda\big(JM^{-1}h - \dot J \dot q\big)}_{\mu} = \Lambda J M^{-1}\tau
	\end{equation*}
	
	As a final step to get our Cartesian-space dynamics, we can assume that the joint torques $\tau$ are generated by a virtual force $f_\tau \in \mathds R^3$ applied at the end-effector; in this way we have that $\tau = J^\top f_\tau$, and the previous equation becomes:
	\begin{equation} \label{eq:temp:3}
	\begin{split}
		\Lambda \ddot x + \mu & = \Lambda J M^{-1} J^\top f_\tau = \big(JM^{-1}J^\top\big)^{-1} J M^{-1} J^\top f_\tau \\ 
		\Lambda \ddot x + \mu & = f_\tau
	\end{split}
	\end{equation}
	Now that we have a Cartesian-space version of the robot dynamics, in the same spirit as the inverse-dynamics control discussed above, we can design a Cartesian-space inverse dynamics controller (a.k.a. Operational-Space Control):
	\begin{equation} \label{eq:cartesiancontroller}
	\begin{cases}
		\tau & = J^\top f_\tau \\
		f_\tau & = \Lambda \ddot x^d + \mu \\
		\ddot x^d & = \ddot x^{ref} + K_d\big(\dot x^{ref} - \dot x\big) + K_p\big(x^{ref} - x\big)
	\end{cases}
	\end{equation}
	A simplified version of this controller can be obtained by compensating for bias forces directly in joint space, rather than in Cartesian space:
	\begin{equation} \label{eq:cartesiancontroller_simp}
	\begin{cases}
		\tau & = J^\top f_\tau \textcolor{red}{+ h}\\
		f_\tau & = \Lambda \ddot x^d
	\end{cases}
	\end{equation}
	To prove the effectiveness of the controller \eqref{eq:cartesiancontroller_simp} we can compute the closed-loop dynamics:
	\begin{align*}
		JM^{-1} \Big( M \ddot q + h & = \tau = J^\top \Lambda \ddot x^d + h \Big) \\
		J \ddot q + JM^{-1} h & = JM^{-1}J^\top \Lambda \  \ddot x^d + JM^{-1} h \\
		J \ddot q & = \ddot x^d \\
		\ddot x - \dot J \dot q & = \ddot x^d
	\end{align*}
	As long as the systems does not move too fast, the term $\dot J \dot q$ is negligible and the end-effectors acceleration $\ddot x$ matches the desired one $\ddot x^d$.
	
	\subsubsection{Redundancy resolution} \label{sssec:redundancy}
	Controller \eqref{eq:cartesiancontroller} aims just to control the 3D motion of the end-effector; however, the number of actuated joints $n$ typically exceeds the 3 degrees of freedom of the end-effector, therefore there are theoretically infinitely many joint configurations that allow to achieve the same end-effector position. The suggested control tracks the desired end-effector position, but it might happen over time that the system reaches \textit{singular configurations} for which $J$ is no longer full-row rank (losing the capability to move in some directions): in this case $JM^{-1}J^\top$ is no longer invertible and the control law cannot be computed.
	
	\textit{Redundancy resolution} aims to solve this problem: since there are infinitely many configurations that allow to achieve a desired $x$, the idea is now to find some additional joint torques that do not impact the end-effector motion, but helps avoiding singular configurations.
	
	Observed that $\Lambda JM^{-1} = \big(JM^{-1}J^\top\big)^{-1}JM^{-1}$ is a \textit{left-inverse} of $J^\top$, i.e. 
	\[ \textrm{let } \big(JM^{-1}J^\top\big)^{-1}JM^{-1} = {J^\top}^\dagger \qquad \textrm{then } {J^\top}^\dagger J^\top = I \]
	Note that in general $J^\top {J^\top}^\dagger \neq I$.
	
	If we now consider any torque $\tau_0$ in the kernel of ${J^\top}^\dagger$ it happens that the dynamics of the system in Cartesian space is not affected at all; recalling \eqref{eq:temp:3} we have that:
	\[ \Lambda \ddot x + \mu = {J^\top}^\dagger\big(\tau + \tau_0\big) = {J^\top}^\dagger \tau \qquad \forall \tau_0 \in \ker\{{J^\top}^\dagger\} \]
	Computing $\tau_0$ can be hard since there is no upfront guarantee that the motion will avoid singular configuration, however we can compute such torques as
	\[ \tau_0 = \big(I - J^\top {J^\top}^\dagger \big) \tau_1 \qquad \forall \tau_1 \in \mathds R^n \]
	Such choice always lies in $\ker\{{J^\top}^\dagger\}$, in fact
	\begin{align*}
		{J^\top}^\dagger \tau_0 & = {J^\top}^\dagger \big(I - J^\top {J^\top}^\dagger\big) \tau_1 \\
		& = \big({J^\top}^\dagger - {J^\top}^\dagger J^\top {J^\top}^\dagger\big) \tau_1 \\
		& = \big({J^\top}^\dagger - {J^\top}^\dagger\big) \tau_1 = 0 \hspace{2cm} \forall \tau_1 \in \mathds R^n
	\end{align*}

	With this premise we can improve controller \eqref{eq:cartesiancontroller} by adding an extra term $\tau_0$ that tries to maintain the joint configuration as close as possible to a reference configuration $q^{ref}$, in order to avoid singular configurations:
	\begin{equation} \label{eq:cartesiancontrollerimproved}
	\begin{cases}
		\tau & = J^\top f_\tau +  \big(I - J^\top {J^\top}^\dagger\big) \tau_1\\
		f_\tau & = \Lambda \ddot x^d + \mu \\
		\ddot x^d & = \ddot x^{ref} + K_d\big(\dot x^{ref} - \dot x\big) + K_p\big(x^{ref} - x\big) \\
		\tau_1 & = M\big(K_{p1}(q^{ref} - q) - K_{d1}\dot q\big) + h
	\end{cases}
	\end{equation}
	 
	
\section{Impedance control}
	Controllers \eqref{eq:cartesiancontroller} and \eqref{eq:cartesiancontrollerimproved} allow to control the robot directly in Cartesian coordinates; such techniques require a very accurate knowledge of the system of interest to work properly.\\
	Model inaccuracies can be compensated to a certain extent by increasing the controller gains, improving the tracking capability; however, this solution can lead to an unsafe stiff system, since in case of contact the controller might generate high forces, which could brake either the robot or the environment.
	
	To overcome this limitation we can use \textit{interaction control}, particularly useful to model contact of the end-effector with the surrounding environment. The underlying idea is that if we have access to an estimate of the contact force, then we can regulate the motion accordingly. Based on this simple idea, there are two main categories of controllers.
	\textit{Direct force controllers} mainly focus on controlling the applied contact forces (not covered in this course).
	\textit{Indirect force controllers} try to enforce a certain dynamic relationship between force and position. This method is also known as \textit{impedance control}.
	
	In impedance control (in Cartesian space) we want the end-effector to behave as a mass-spring-damper subject to an external contact force $f$:
	\begin{equation} \label{eq:spring}
		M_d \ddot x + B \dot x + K x = f
	\end{equation}
	where $M_d, B, K$ are matrices defining the desired mass, damping and stiffness.
	
	Once the contact model \eqref{eq:spring} is defined, the controller needs to compute at each time instant the joint torques $\tau$ to follow the desired behavior. 
	Starting from \eqref{eq:temp:3}, the dynamics projected in Cartesian space is simply 
	$$
	\Lambda \ddot x + \mu = {J^\top}^\dagger \tau + f
	$$
	Solving explicitly \eqref{eq:spring} for the acceleration provides the desired value 
	$$
	\ddot x^d = M_d^{-1}\big(f-B\dot x - Kx\big),
	$$ 
	which substituted in the dynamics:
	\[ \Lambda M_d^{-1}\big(f - B\dot x - K x\big) + \mu = {J^\top}^\dagger \tau + f \]
	As we did for the Operational-Space Control, assuming the joint torques $\tau$ are generated by a virtual end-effector force $f_\tau$, then the previous equation can be solved for $f_\tau$ as:
	\begin{align*}
		f_\tau & = \Lambda M_d ^{-1} \big( f - B\dot x - Kx \big) + \mu - f \\
		& = -\Lambda M_d^{-1}\big(B\dot x + K x\big) + \mu + \big(\Lambda M_d^{-1} - I\big) f
	\end{align*}
	At this point in order to compute $\tau$ (from $f_\tau$) it's required an estimate of the contact force $f$ that, unfortunately, usually comes from very noisy sensors and with time delays that might lead to unstable systems. A trick that we can use to overcome this limitation is to set $M_d = \Lambda$ in the desired dynamics \eqref{eq:spring}: with this choice, $f_\tau$ no longer depends on the contact force $f$ (since $\Lambda M_d^{-1} = I$). 
	At the end, the resulting control law simplifies to:
	\begin{equation}
		\tau = J^\top f_\tau = J^\top \big(-Kx - B\dot x + \mu\big)
	\end{equation}
	Similar derivations can be carried out even for cases where a reference trajectory and a reference contact force have to be tracked, by simply modifying the desired dynamics in this way:
	\begin{equation} \label{eq:spring}
		M_d (\ddot x^{ref} - \ddot x) + B (\dot x^{ref} - \dot x) + K (x^{ref} - x) = f^{ref} - f
	\end{equation}
	
\section{QP-based reactive control}
	Up to this point, we have discussed only classic control methods, mainly from the 60's-80's, which do not rely on numerical optimization. In this section we turn our attention to \textit{optimization-based approaches}, which exploit numerical optimization solvers to determine the optimal control inputs. The main advantage of these methods is that they can account for constraints (such as actuator effort, joint position and velocity limits), which classic approaches could not consider.
	Before delving into this new class of control methods, let us quickly review the kind of convex optimization problems that will be used in this section.
	
	\subsection{Taxonomy of convex optimization problems} \label{sec:qp}
	In general not all optimization problems can be solved; moreover, for control purposes, the execution time of numerical algorithms is fundamental since we want to control the system with as little delay as possible.
	In this section, we are mainly concerned with two kinds of convex optimization problems: i) Least-Squares Programs (LSP's) and ii) Quadratic Programs (QP's) \footnote{QP's can be either convex or not, but for the rest of this section we will restrict our discussion to convex QP's.}.
	LSP's are problem with affine equality/inequality constraints and a cost function that is the squared norm of an affine function:
	\begin{eqs}{eq:lsp}
	\minimize_{x} \quad & \frac{1}{2} || A x - b||^2 \\
	\st \quad & A_{eq} x = b_{eq} \\
	& A_{in} x \le b_{in}
	\end{eqs}
	QP's admit the same constraints as LSP's, but a more general kind of cost (i.e. quadratic):
	\begin{eqs}{eq:qp}
	\minimize_{x} \quad & \frac{1}{2} x^\T H x + g^\T x \\
	\st \quad & A_{eq} x = b_{eq} \\
	& A_{in} x \le b_{in}
	\end{eqs}
	The QP is convex if and only if the Hessian matrix $H$ is positive semi-definite (i.e. all its eigenvalues have non-negative real part), which implies:
	\begin{eqs}{eq:psd_matrix}
	x^\T H x \ge 0 \qquad \forall x
	\end{eqs}
	LSP's are a subclass of QP's, which can be clearly seen by expanding the cost function of~\eqref{eq:lsp}:
	\begin{eqs}{eq:lsp_cost}
	\half || A x - b ||^2 = \half x^\T \underbrace{A^\T A}_H x \underbrace{- b^\T A}_{g^\T} x + \half b^\T b,
	\end{eqs}
	where we have highlighted the connection with the Hessian matrix $H$ and gradient vector $g$ of the QP's cost.
	The Hessian of an LSP's cost is always positive semi-definite because the quadratic term can be interpreted as a norm of another vector $y \triangleq A x$, which cannot be negative:
	\begin{eqs}{eq:lsp_hessian}
	x^\T A^\T A x = y^\T y = || y ||^2 \ge 0 \qquad \forall x
	\end{eqs}
	The reason why the class of LSP's is strict contained in the class of QP's is because of the gradient vector, which cannot be arbitrarely chosen in an LSP, but must be related to the problem's Hessian through the matrix $A$: $g = A^\T b$. 
	Indeed, this relationship implies that $g$ must be a linear combination of the rows of $A$.
	
	Both LSP's and QP's are convex optimization problem that can be solved sufficiently fast to be used inside fast control loops (i.e. with control frequencies between 100 and 1000 Hz). 
	Moreover, since LSP's are a special kind of QP's, QP solvers can be used also to solve LSP's. 
	Indeed, since QP solvers are more common than LSP solvers, often times people use QP solvers to solve LSP's.
	
	\subsubsection{Solving Unconstrained LSP's}
	Since LSP's are convex, then any stationary point $x^\star$ of the cost function $f(x)$ is also a global minimizer of the cost.
	In case the LSP has no constraints, $x^\star$ can be computed by setting to zero the gradient of the cost $f$, i.e. solving:
	\[ \nabla f = A^\top A x - A^\top b = 0 \]
	Assuming that $A^\top A$ is invertible the explicit solution of the minimizing argument is
	\begin{equation*} \tag{$\dagger$}
		x^\star = \big(A^\top A\big)^{-1} A^\top b
	\end{equation*}
	where $\big(A^\top A\big)^{-1} A^\top$ is the \textit{left pseudo-inverse} of $A$. If $A^\top A$ is not invertible but conversely $AA^\top$ is, then we can use the \textit{right pseudo-inverse} to obtain the solution
	\begin{equation*} \tag{$\ddagger$}
		x^\star = A^\top \big(AA^\top\big)^{-1} b
	\end{equation*}
	We can verify that this is indeed a minimizer of $f$ by substituting it in the cost function, and checking that it leads in fact to a zero (minimum) cost:
	\[ f(x^\star) =\big\| AA^\top\big(AA^\top\big)^{-1} b - b \big\|^2 = \|b-b\|^2 = 0 \]
	
	In general, the solution of least-squares minimization problem is determined by means of the \textit{Moore-Penrose pseudo-inverse $A^\dagger$} as
	\begin{equation} \label{eq:lspsol}
		x^\star = \arg \min_x \big\| Ax - b \big\|^2 = A^\dagger b
	\end{equation}
	where
	\begin{equation}
		A^\dagger = \begin{cases}
			A^\top\big(AA^\top\big)^{-1} & \textrm{if $A$ is full-row rank} \\
			\big(A^\top A\big)^{-1}A^\top & \textrm{if $A$ is full-column rank} 
		\end{cases}
	\end{equation}
	
	We observe now that both solutions have been obtained considering the respective definition of the Moore-Penrose pseudo-inverse based on the condition of the matrix $A$ (leading to the invertibility of $A^\top A$ or $A A^\top$). 
	Even if $A$ is neither full-row and nor full-column rank, its pseudo-inverse $A^\dagger$ can still be computed by means of a matrix decomposition (e.g. Singular Value Decomposition).
	

	
\subsection{Joint-space control}
	To track a reference trajectory in joint space $q^{ref}$, we have already discussed the inverse-dynamics controller \eqref{eq:inversedynamiccontrol}, which is:
	\[ \tau = M \ddot q^d + h \qquad\textrm{with } \ddot q^d = \ddot q^{ref} + K_d \dot e + K_p e \]
	%
	The same result can be achieved by solving the following LSP:
	\begin{equation} \label{eq:qpocp}
	\begin{split}
		\minimize_{\tau, \ddot q}\quad & \frac 1 2 \big\|\ddot q - \ddot q^d \big\|^2 = \frac 1 2 \big(\ddot q - \ddot q^d\big)^\top \big(\ddot q - \ddot q^d\big) \\
		\st \quad & M \ddot q + h = \tau
	\end{split}
	\end{equation}
	where $\frac 1 2 \big\|\ddot q - \ddot q^d \big\|^2$ is the \textit{cost function} that needs to be minimized with respect to the \textit{decision variables} $\ddot q, \tau$, while satisfying the \textit{constraint} $M\ddot q + h = \tau$ (i.e. the system dynamics).
	
	Since in \eqref{eq:qpocp} the cost is a squared norm, the minimum is obtained when the argument of the norm is zero, i.e. the optimal acceleration is $\ddot q^\star = \ddot q^d$; this implies that the optimal commanded torques are simply $\tau^\star = M \ddot q^d + h$, as in \eqref{eq:inversedynamiccontrol}.
	
	In this case the solution of the minimization problem was trivial, thus applying \eqref{eq:inversedynamiccontrol} would have been easier, however the formulation \eqref{eq:qpocp} leaves room for adding other constraints, which until now were impossible to consider. 
		
	\paragraph{Actuator effort bounds} Real actuators can't exert infinitely high forces, but their actions present a saturation limit that acts as a bound. Considering a symmetric behavior on the forces that can be generated, the actuator effort can be bounded as
	\[ -\tau^{max} \leq \tau \leq \tau^{max} \]
	
	Furthermore while dealing with electrical actuators, the motor drivers can handle just a limited amount of current, thus limiting the overall torque that can be generated.
	Since the motor current $i$ is proportional to the motor torque $\tau$, this limitation can be described by the following inequality constraint:
	\[ -i^{max} \leq \underbrace{K_i \tau}_{i} \leq i^{max} \]
	
	\paragraph{Joint velocity bounds} Motors and gears also have limited velocities $\dot q^{max}$. We observe that in \eqref{eq:qpocp} the joint velocity $\dot q$ is not a decision variable, but it is a constant because the control inputs cannot affect the current robot velocity. 
	Since we can't act on the current velocity, we can ensure that at the next time step the velocity doesn't exceed its limit. 
	Assuming joint accelerations remain constant during the time step, which is reasonable because time steps are usually very short in reactive control schemes, we have that $\dot q(t + \Delta t) = \dot q(t) + \Delta t \, \ddot q$, thus a constraint that we might add to account for velocity limits is:
	\[ -\dot q^{max} \leq \dot q + \Delta t \, \ddot q \leq \dot q^{max} \]
	
	\paragraph{Joint position bounds} Actuators also have bounded joint positions, i.e. $q $ is constrained to lie in a interval $\big[q^{min}, q^{max}\big]$. 
	To satisfy such constraint we might be tempted to exploit the same trick used for the joint velocity limits, i.e. to compute $q$ at the next time-step by integrating $\ddot q$ twice, reaching a formulation of the form
	\[ q^{min} \leq q + \Delta t\, \dot q + \frac 1 2 \Delta t^2 \, \ddot q \leq q^{max} \tag{$\circ$}\]
	
	This solution would work if we only had joint position and velocity limits. 
	However, since in practice we also have joint acceleration limits (which are a direct consequence of the joint torque limits), this approach can lead to an \textit{unfeasible QP}.
	This is because in order not to exceed the joint position limits, the controller might be required to produce a "high" (in absolute value) acceleration $\ddot q$ that is incompatible with the actuator capabilities.
	
	A heuristic that can be used to mitigate this issue is to use the constraint ($\circ$) with a larger value for the time-step $\Delta t$.
	This has the effect of "looking further into the future", and thus requiring the joint to decelerate more in advance.
	However, this is not sufficient in general to ensure feasibility.
	An in-depth discussion of this issue can be found in the paper~\cite{DelPreteRAL2018}, where an exact solution is suggested, under the assumption of constant joint acceleration bounds.
	We will come back to this issue when discussing Model Predictive Control methods, which are more suited to account for this kind of state constraints.
	
	
	\paragraph{QP problem} 
	The optimal control problem in \eqref{eq:qpocp} can be extended in order to embed all the bounds described so far, reaching the formulation:
	\begin{equation} \label{eq:qp}
		\begin{split}
			\minimize_{\tau, \ddot q}\quad & \frac 1 2 \big\|\ddot q - \ddot q^d \big\|^2 = \frac 1 2 \big(\ddot q - \ddot q^d\big)^\top \big(\ddot q - \ddot q^d\big) \\
			\st \quad & M \ddot q + h = \tau \\
			& -\tau^{max} \leq \tau \leq \tau^{max} \\
			& -i^{max} \leq K_i \tau \leq i^{max} \\
			& -\dot q^{max} \leq \dot q + \Delta t \, \ddot q \leq \dot q^{max} \\
			& q^{min} \leq q + \Delta T\, \dot q + \frac 1 2 \Delta T^2 \, \ddot q \leq q^{max}
		\end{split}
	\end{equation}
	Such problem is harder to solve than \eqref{eq:qpocp} because its solution is usually non-trivial. 
	For this reason, \textit{QP solvers} are typically used, which are customized software to solve convex QP problems as~\eqref{eq:qpocp}.
	

\subsection{Cartesian-Space Control}
	We can use quadratic programs also to solve control problems in Cartesian space. 
	Recalling the desired Cartesian acceleration $\ddot x^d = \ddot x^{ref} + K_p\big(x^{ref}-x\big) + K_d\big(\dot x^{ref} - \dot x\big)$ defined in \eqref{eq:cartesiancontroller}, and recalling the relationship between Cartesian-space and joint space accelerations $\ddot{x} = J\ddot{q} + \dot{J} \dot{q}$, we can formulate the problem as:
	\begin{equation}
	\begin{aligned}
		\minimize_{\tau, \ddot q, \ddot x} \quad & \frac12 \big\|\ddot x - \ddot x^d\|^2 \\
		\st \quad & M\ddot q + h = \tau \\
		& \ddot x = J\ddot q + \dot J \dot q
	\end{aligned}
	\end{equation}
	To improve the numerical performance of the algorithm we can remove the last constraint and explicitly substitute it in the cost function:
	\begin{equation} \label{eq:qpcartesian}
		\begin{aligned}
			\minimize_{\tau, \ddot q} \quad & \frac12 \big\| J\ddot q + \dot J\dot q - \ddot x^d\|^2 \\
			\st \quad & M\ddot q + h = \tau
		\end{aligned}
	\end{equation}
	Exploiting (\ref{eq:lspsol}) we can see that the optimal joint accelerations and torques are computed simply as
	\begin{eqs}{eq:cartesian_qp_sol}
	\ddot q^\star = J^\dagger \big(\ddot x^d - \dot J\dot q\big) \qquad \tau^\star = M \ddot q^\star + h
	\end{eqs}
	where in this case the coefficient $b$ is equal to $\ddot x^d - \dot J \dot q$ (all these quantities are constant in the problem).
	Notice that by using a weighted pseudo-inverse\footnote{A weighted pseudo-inverse of a full-row rank matrix $A$, with weight matrix $W$, is defined as $W A^\T (A W A^\T)^{-1}$.} in \eqref{eq:cartesian_qp_sol} with $M^{-1}$ as weight matrix, we would recover the Operational-Space Control law.
	
%	\textbf{TODO: confrontare risultato con quello ottenuto in precedenza (pseudoinversa pesata)}
	
	
\subsection{Task-Space Control}
	In this section we generalize the concept of Cartesian-space control so that we can control not just the end-effector in Cartesian coordinates, but any \textit{task} (a.k.a. a \textit{control objective}) that the robot should achieve.
	
	Each task is usually described by an \textit{task function} $e(x, u, t)$, where $x = (q,\dot q)$ is the \textit{state} of the system, and $u=\tau$ the inputs (using a more general notation).
	The task function must be defined by the user so that:
	\begin{itemize}
	\item $e=0$ implies that the task has been achieved;
	\item the closer $e$ is to zero, the closer we are to achieving the task;
	\item $e$ is sufficiently smooth to be differentiable for the required number of times.
	\end{itemize}
	Without loss of generality, let us assume that the error function is defined as the difference between a given time-invariant output function $y(x,u)$ and a time-varying reference value $y^{ref}(t)$:
	\begin{equation}
		e(x,u,t) = y(x,u) - y^{ref}(t)
	\end{equation}
	Since LSP's can only minimize the norm of an affine function in the decision variable $\ddot q$, $\tau$, a transformation is required (as the one to solve the problem in Cartesian space); minimizing directly $e$ is not possible as in general it depends only on $q$ and $\dot q$, and not on $\ddot q$.
	To reach this goal we can try to control how $e$ evolves in time, by imposing the following two conditions:
	\begin{enumerate}
		\item $\lim_{t\rightarrow \infty} e(t) = 0$;
		\item the dynamics of $e$ is affine in the decision variables $\ddot q, u$.
	\end{enumerate}
	To see how we can meet these conditions we analyze separately three cases, depending on whether the output function $y$ depends on $q$, $\dot q$, or $\tau$.
	
	\subsubsection{Position task function} 
	Consider a task depending only on the joint positions $q$, in the form $e(x,u,t) = e(q,t) = y(q) - y^{ref}(t)$.
	An affine function in $\ddot q$ can be obtained by differentiation $e$ in time for two times:
	\[ \xrightarrow{\d/\d t} \qquad \dot e = J\dot q - \dot y^{ref} \qquad \xrightarrow{\d/\d t} \qquad \ddot e = J \ddot q + \dot J \dot q - \ddot y^{ref} \tag{$\circ$}  \]
	In this way the second requirement for $e$ is satisfies. 
	Now we need to choose a control policy for $\ddot{e}$ so that $e$ converges to zero.
	To do that we reduce the dynamic to a first-order differential equation. 
	Introducing the augmented state variable $z \triangleq (e,\dot e)$, and considering for this case a proportional-derivative controller, the linear system boils down to:
	\[ \dot z = \begin{pmatrix}
		\dot e \\ \ddot e
	\end{pmatrix} = \begin{bmatrix}
		0 & I \\ -K_p & - K_d
	\end{bmatrix} \begin{pmatrix}
		e \\ \dot e
	\end{pmatrix} = A z  \]
	In this case matrices $K_p,K_d > 0$ must be chosen in such a way that $\ddot e = -K_p e - K_d \dot e$ is asymptotically stable ($A$ musth be Hurwitz). 
	Since now also the first condition is met, we can rewrite ($\circ$) as
	\begin{align*}
		J\ddot q + \dot J \dot q - \ddot y^{ref} & = - K_p e - K_d \dot e \\
		J\ddot q + \dot J \dot q + K_p e + K_d \dot e & = 0
	\end{align*}
	Linear system theory tells us that such dynamic is exponentially stable, however in this case no constraints have been considered (but they can limit the actual stabilization performance).


	\subsubsection{Velocity task function} 
	Let us consider a case for which the error function depends just on the joint velocity, so in the form $e(x,u,y) = e(\dot q, t) = y(\dot q) - y^{ref}(t)$; differentiating in time the error function provides
	\[ \dot e(\dot q, t) = \dot y - \dot y^{ref} = \pdiff y {\dot q} \diff {\dot q} t - \dot y^{ref} = J \ddot q - \dot y^{ref} \]
	This expression clearly satisfies the second requirement of being affine with respect to $\ddot q$; to ensure the second condition we can set $\dot e$ as proportional to the current error:
	\[ \dot e = -K_p e \]
	If $K_p$ is positive definite the linear dynamics is asymptotically stable, satisfying the second condition. The final cost function that can be used to solve this problem is so
	\begin{equation} 
	\begin{aligned} 
		J \ddot q - \dot y^{ref} & = - K_p e \qquad \textrm{with } K_p > 0 \\ 
		J\ddot q - \dot y^{ref} + K_p e & = 0
	\end{aligned}
	\end{equation}
	
	
	\subsubsection{Input task function} The only way to build a task function $e(u, t)$ that depends just on the input torques $\tau = u$ is by ensuring that $e$ is actually affine in $u$ (which is a decision variable), i.e. must be of the form
	\[ e(u,t) = A(t) u - b(t) \]
	
	\subsubsection{Task summary} As just presented we might have 3 main kinds of task functions:
	\begin{itemize}
		\item affine functions of the input $\tau = u$;
		\item nonlinear functions of the state $x$, for which an affine function of $\ddot q$ can be found by means of one/two differentiations in time for velocity and position tasks, respectively.
	\end{itemize}
	In general, regardless of which task function we are working with, we can express any task function as an affine function of the decision variables $z = (\ddot q, u)$:
	\[ g(z) = \begin{bmatrix}
		A_x & A_u
	\end{bmatrix} \begin{pmatrix}
		\ddot q \\ u
	\end{pmatrix} - b = Az - b \]
	
	\paragraph{QP-based control}  \textit{Task-Space Inverse Dynamics} (\textit{TSID}), also referred TO as \textit{QP-based control}, solves optimization problems in the form:
	\begin{equation}
	\begin{aligned}
		\minimize_{z =(\ddot q, \tau)} \quad & \big\|Az-b \big\|^2 \\
		\st \quad & \begin{bmatrix}
			M & -I
		\end{bmatrix} z = -h \\ 
		& \text{other constraints}
	\end{aligned}
	\end{equation}
	This is a generalization of the QP joint-space control in (\ref{eq:qp}), choosing $A = \begin{bmatrix} I & 0 \end{bmatrix}$ and $b = \ddot q^d$, or the Cartesian-space control in (\ref{eq:qpcartesian}), choosing $A = \begin{bmatrix} J & 0 \end{bmatrix}$ and $b = \ddot x^d - \dot J \dot q$.
	
\subsection{Underactuated systems}
TSID can easily deal with underactuated systems, which are systems that have less control inputs than degrees of freedom.
So far, we have focused on robot manipulators, which are fully actuated because typically each joint is actuated by a motor.
However, most mobile robots are underactuated. 
Let us take, for instance, vehicles.
Assuming a vehicle cannot detach from the ground, its configuration can be described with 3 variables: 2 for the position in the plane, and 1 for the orientation.
However, a vehicle has typically only 2 actuators: the motor (which is commanded through the accelerator pedal) and the steering wheel.
Therefore it is underactuated.
The same applies to most aerial vehicles, which have 6 degrees of freedom, and typically 2-to-4 actuators.
Underwater robots and legged robots are other examples of typically underactuated systems.

Many underactuated robots can be modeled with a small modification to the dynamics equations of fully-actuated systems.
Let us assume that we remove some motors from a robot manipulator, making it underactuated. 
The effect that this would have on its dynamics is that the torque at the corresponding joints would be alway zero. 
We can model this by introducing a selection matrix $S \in \R{n_a}{n}$, which selects the actuated joints of the vector $q$:
\begin{eqs}{}
q_{actuated} = S q = S \mat{q_{passive} \\ q_{actuated}}
\end{eqs}
The same matrix can be also used to modify the dynamics as:
\begin{eqs}{}
M \ddot q + h = S^\T \tau = \mat{0 \\ \tau}
\end{eqs}
We can see that the dynamics is still affine with respect to $\ddot q$ and $\tau$, therefore we can still use it as a constraint in TSID's QP.


\subsection{Rigid Contacts}
Rigid contacts are particularly hard to deal with because they constrain the motion of the robot.
Let us suppose that a robot manipulator makes contact with a wall, using its end-effector.
After making contact, the end-effector can no longer move in the direction of the wall, and if it tries, it may apply a large contact force that could lead to unpleasant consequences (either for the robot or for the wall).
Therefore, we need the controller to be aware of this constraint. 
This constraint could be modeled as an inequality constraint on the robot configuration, representing the fact that the position of the end-effector (or whatever point the robot is using for making contact) cannot penetrate the wall:
\begin{eqs}{}
c(q) \le \text{constant}
\end{eqs}
However, very often, once the robot has made contact with the environment, we want this contact to be maintained for a certain amount of time. 
Therefore, for this time period, the contact constraint can be modeled using an equality constraint, representing the fact that the contact point(s) should not move at all:
\begin{eqs}{}
c(q) = \text{constant}
\end{eqs}
At this point, we should be familiar with the fact that we cannot include in TSID a constraint that depends only on $q$.
Similarly to what we have already done for the task functions, we need to differentiate this constraint until it becomes a function of $\ddot q$:
\begin{eqs}{}
J_c \dot q = 0 \\
J_c \ddot q + \dot{J}_c \dot q = 0
\end{eqs}
Besides modeling the motion constraints, rigid contacts also affect the system dynamics by means of the associated contact forces.
Therefore, to account for rigid contact in TSID we need to introduce the contact forces $\lambda$ as decision variables, and account for them in the system dynamics.
In conclusion, the QP problem for an underactuated system in rigid contact with the environment is:
\begin{equation}
	\begin{aligned}
		\minimize_{z =(\ddot q, \tau, \lambda)} \quad & \big\|Az-b \big\|^2 \\
		\st \quad & 
		\begin{bmatrix}
			M & -S^\T & J_c^\T \\ 
			J_c & 0 & 0
		\end{bmatrix} 
		z 
		= 
		\mat{ -h \\ -\dot{J}_c \dot q} \\
		& \text{other constraints}
	\end{aligned}
	\end{equation}



\subsection{Multi-Task Control}
Complex robots typically need to carry out multiple tasks at the same time.
This is often the case even for robot manipulators because they typically have more degrees of freedom than the dimension of the main task they must perform.
For instance, if we want to control the position and orientation of the end-effector of a robot manipulator, then the task has dimension 6.
However, many robot manipulators have 7 degrees of freedom, which means that they are \emph{redundant} with respect to the task.
In this case, it is recommended to introduce a secondary task in the control framework, to avoid that the uncontrolled degree(s) of freedom lead the robot state to drift and diverge, typically hitting joint limits or reaching undesirable configurations with low dexterity.

Assume a robot is required to perform $N$ tasks, and each task is associated with a cost function $g_i$ defined as:
\begin{eqs}{}
g_i(z) = || A_i z - b_i ||^2, \qquad i=1, \dots, N
\end{eqs}
We can then achieve a multi-task control by formulating TSID's QP problem as:
 \begin{equation}
	\begin{aligned}
		\minimize_{z =(\ddot q, \tau, \lambda)} \quad & \sum_{i=1}^N w_i \, g_i(z) \\
		\st \quad & 
		\begin{bmatrix}
			M & -S^\T & J_c^\T \\ 
			J_c & 0 & 0
		\end{bmatrix} 
		z 
		= 
		\mat{ -h \\ -\dot{J}_c \dot q} \\
		& \text{other constraints},
	\end{aligned}
	\end{equation}
where $w_i \in \Rv{}$ is a user-defined positive weight associated to task $i$.
These weights can be used to define the relative importance between tasks.
Higher priority tasks will be assigned higher weights, whereas lower priority tasks will have lower weights.
A ratio of $10^3$ between the weights of two tasks typically leads in practice to a behavior that is indistinguishable from the one obtained with the null-space projection method used with Operational-Space control (Section~\ref{sssec:redundancy}).

	